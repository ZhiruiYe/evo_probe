{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e73b60a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"/yezhirui/evo_probe\")\n",
    "from src.find_contact import *\n",
    "from src.gmm import *\n",
    "from src.contact_space import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07bf9dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 初始化数据和模型 ===\n",
      "PDB坐标提取完成: 93 个残基 (链 A)\n",
      "PDB坐标提取完成: 60 个残基 (链 A)\n",
      "距离矩阵计算完成: 93 x 93\n",
      "距离矩阵计算完成: 60 x 60\n",
      "共同残基数量: 60 个 (原始: 93 vs 60)\n",
      "从PDB提取序列完成: 93 个残基 (链 A)\n",
      "从PDB提取序列完成: 60 个残基 (链 A)\n",
      "自动检测到PDB起始偏移量: 3\n",
      "警告: PDB序列比MSA序列长，有24个PDB残基未映射\n",
      "PDB到MSA映射完成: 66 个残基 (匹配: 66, 不匹配: 0)\n",
      "PDB序列长度: 93, MSA序列长度: 70\n",
      "PDB起始偏移: 3 (PDB第4个残基对应MSA第0个位置)\n",
      "自动检测到PDB起始偏移量: 3\n",
      "警告: MSA序列比PDB序列长，MSA位置61(R)无对应PDB残基\n",
      "PDB到MSA映射完成: 57 个残基 (匹配: 57, 不匹配: 0)\n",
      "PDB序列长度: 60, MSA序列长度: 70\n",
      "PDB起始偏移: 3 (PDB第4个残基对应MSA第0个位置)\n",
      "接触对转换完成: 109 个有效接触对, 2 个跳过\n",
      "接触对转换完成: 133 个有效接触对, 0 个跳过\n",
      "接触对转换完成: 1354 个有效接触对, 172 个跳过\n",
      "关键接触点数量: 242\n"
     ]
    }
   ],
   "source": [
    "print(\"=== 初始化数据和模型 ===\")\n",
    "\n",
    "# 1. 获取关键接触点\n",
    "chemokine_pdb = \"1j8i\"  # 趋化因子折叠\n",
    "alternate_pdb = \"2jp1\"  # 替代折叠\n",
    "msa_seq = 'EVSDKRT-CVSLTTQRLPVSRIKTYTIT---EGSLRAVIFITKRGLKVCADPQATWVRDVVRSMDRKSNT'\n",
    "results = calculate_contact_difference_msa_id(chemokine_pdb, alternate_pdb,msa_seq,threshold=10.0,remove_diag=5)\n",
    "critical_contacts = results['critical_contacts']\n",
    "print(f\"关键接触点数量: {len(critical_contacts)}\")\n",
    "\n",
    "# 2. 加载MJ矩阵\n",
    "mj_dict = load_mj_matrix(\"/yezhirui/evo_probe/data/mj_matrix.txt\")\n",
    "\n",
    "# 3. 生成contact embedding\n",
    "\n",
    "contact_space = ContactSpace(critical_contacts, mj_dict)\n",
    "\n",
    "# 批量添加节点\n",
    "path_dir = \"/yezhirui/evo_probe/data/sample\"\n",
    "node_configs = [(\"ANC0\", f\"{path_dir}/node499_anc0_samples.fasta\"),(\"ANC1\", f\"{path_dir}/node500_anc1_samples.fasta\"),\n",
    "(\"ANC2\", f\"{path_dir}/node501_anc2_samples.fasta\"), (\"ANC3\", f\"{path_dir}/node502_anc3_samples.fasta\"), (\"ANC4\", f\"{path_dir}/node507_anc4_samples.fasta\")]\n",
    "for node_id, fasta_path in node_configs:\n",
    "    contact_space.add_node_from_fasta(node_id, fasta_path)\n",
    "\n",
    "contact_space.build_embeddings()\n",
    "\n",
    "# anc0_contact_embedding = contact_space.get_node_embeddings(\"ANC0\")\n",
    "# anc1_contact_embedding = contact_space.get_node_embeddings(\"ANC1\")\n",
    "anc2_contact_embedding = contact_space.get_node_embeddings(\"ANC2\")\n",
    "anc3_contact_embedding = contact_space.get_node_embeddings(\"ANC3\")\n",
    "anc4_contact_embedding = contact_space.get_node_embeddings(\"ANC4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d16c49f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "# 联合聚类方法\n",
    "def joint_clustering_analysis(datasets, k=2):\n",
    "    \"\"\"对所有数据集进行联合聚类分析\"\"\"\n",
    "    # 合并所有数据\n",
    "    all_data = []\n",
    "    dataset_labels = []\n",
    "    \n",
    "    for name, data in datasets.items():\n",
    "        all_data.append(data)\n",
    "        dataset_labels.extend([name] * len(data))\n",
    "    \n",
    "    combined_data = np.vstack(all_data)\n",
    "    \n",
    "    # 在合并数据上进行聚类\n",
    "    gmm = GaussianMixture(n_components=k, random_state=42)\n",
    "    cluster_labels = gmm.fit_predict(combined_data)\n",
    "    \n",
    "    # 分析每个数据集在各cluster中的分布\n",
    "    results = {}\n",
    "    start_idx = 0\n",
    "    for name, data in datasets.items():\n",
    "        end_idx = start_idx + len(data)\n",
    "        dataset_clusters = cluster_labels[start_idx:end_idx]\n",
    "        results[name] = {\n",
    "            'cluster_labels': dataset_clusters,\n",
    "            'cluster_distribution': np.bincount(dataset_clusters)\n",
    "        }\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    return results, gmm\n",
    "\n",
    "\n",
    "\n",
    "# 比较不同数据集的聚类中心\n",
    "def compare_cluster_centers(results_dict):\n",
    "    \"\"\"比较不同数据集的聚类中心相似性\"\"\"\n",
    "    centers = {}\n",
    "    for dataset_name, result in results_dict.items():\n",
    "        gmm = result['gmm']\n",
    "        centers[dataset_name] = gmm.means_\n",
    "    \n",
    "    # 计算中心间距离矩阵\n",
    "    from scipy.spatial.distance import cdist\n",
    "    \n",
    "    for name1, center1 in centers.items():\n",
    "        for name2, center2 in centers.items():\n",
    "            if name1 != name2:\n",
    "                # 计算所有可能的cluster配对距离\n",
    "                distances = cdist(center1, center2)\n",
    "                print(f\"{name1} vs {name2}:\")\n",
    "                print(f\"  最小距离配对: {np.min(distances)}\")\n",
    "                print(f\"  距离矩阵:\\n{distances}\")\n",
    "\n",
    "\n",
    "def analyze_conformational_similarity(datasets, all_results):\n",
    "    \"\"\"分析不同数据集cluster间的构象倾向性相似性\"\"\"\n",
    "    \n",
    "    # 对于每对数据集，比较其cluster的构象特征\n",
    "    dataset_names = list(datasets.keys())\n",
    "    \n",
    "    for i, name1 in enumerate(dataset_names):\n",
    "        for j, name2 in enumerate(dataset_names[i+1:], i+1):\n",
    "            print(f\"\\n比较 {name1} 和 {name2}:\")\n",
    "            \n",
    "            # 获取各cluster的平均构象特征\n",
    "            data1 = datasets[name1]\n",
    "            data2 = datasets[name2]\n",
    "            labels1 = all_results[name1]['cluster_labels']\n",
    "            labels2 = all_results[name2]['cluster_labels']\n",
    "            \n",
    "            # 计算每个cluster的中心\n",
    "            center1_0 = data1[labels1 == 0].mean(axis=0)\n",
    "            center1_1 = data1[labels1 == 1].mean(axis=0)\n",
    "            center2_0 = data2[labels2 == 0].mean(axis=0)\n",
    "            center2_1 = data2[labels2 == 1].mean(axis=0)\n",
    "            \n",
    "            # 计算交叉相似性\n",
    "            sim_00 = np.corrcoef(center1_0, center2_0)[0,1]\n",
    "            sim_01 = np.corrcoef(center1_0, center2_1)[0,1]\n",
    "            sim_10 = np.corrcoef(center1_1, center2_0)[0,1]\n",
    "            sim_11 = np.corrcoef(center1_1, center2_1)[0,1]\n",
    "            \n",
    "            print(f\"  {name1}_cluster0 vs {name2}_cluster0: {sim_00:.3f}\")\n",
    "            print(f\"  {name1}_cluster0 vs {name2}_cluster1: {sim_01:.3f}\")\n",
    "            print(f\"  {name1}_cluster1 vs {name2}_cluster0: {sim_10:.3f}\")\n",
    "            print(f\"  {name1}_cluster1 vs {name2}_cluster1: {sim_11:.3f}\")\n",
    "            \n",
    "            # 判断最佳匹配\n",
    "            if max(sim_00, sim_11) > max(sim_01, sim_10):\n",
    "                print(f\"  最佳匹配: 标签一致 (0-0, 1-1)\")\n",
    "            else:\n",
    "                print(f\"  最佳匹配: 标签交换 (0-1, 1-0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cd16bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "联合聚类结果:\n",
      "ANC2:\n",
      "  cluster分布: [1000]\n",
      "  总样本数: 1000\n",
      "  cluster0占比: 1.000 (1000个样本)\n",
      "  ⚠️  警告: ANC2的所有数据都被分到了同一个cluster!\n",
      "ANC3:\n",
      "  cluster分布: [ 21 979]\n",
      "  总样本数: 1000\n",
      "  cluster0占比: 0.021 (21个样本)\n",
      "  cluster1占比: 0.979 (979个样本)\n",
      "ANC4:\n",
      "  cluster分布: [   0 1000]\n",
      "  总样本数: 1000\n",
      "  cluster0占比: 0.000 (0个样本)\n",
      "  cluster1占比: 1.000 (1000个样本)\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    'ANC2': anc2_contact_embedding,\n",
    "    'ANC3': anc3_contact_embedding,\n",
    "    'ANC4': anc4_contact_embedding\n",
    "}\n",
    "\n",
    "\n",
    "joint_results, joint_gmm = joint_clustering_analysis(datasets, k=2)\n",
    "\n",
    "# 查看联合聚类结果（安全版本）\n",
    "print(\"\\n联合聚类结果:\")\n",
    "for dataset_name, result in joint_results.items():\n",
    "    cluster_dist = result['cluster_distribution']\n",
    "    total_samples = sum(cluster_dist)\n",
    "    \n",
    "    print(f\"{dataset_name}:\")\n",
    "    print(f\"  cluster分布: {cluster_dist}\")\n",
    "    print(f\"  总样本数: {total_samples}\")\n",
    "    \n",
    "    # 安全地访问cluster比例\n",
    "    for i in range(len(cluster_dist)):\n",
    "        ratio = cluster_dist[i] / total_samples\n",
    "        print(f\"  cluster{i}占比: {ratio:.3f} ({cluster_dist[i]}个样本)\")\n",
    "    \n",
    "    # 检查是否所有数据都在一个cluster中\n",
    "    if len(cluster_dist) == 1:\n",
    "        print(f\"  ⚠️  警告: {dataset_name}的所有数据都被分到了同一个cluster!\")\n",
    "    elif len(cluster_dist) < 2:\n",
    "        print(f\"  ⚠️  警告: {dataset_name}只有{len(cluster_dist)}个cluster，少于预期的2个\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
